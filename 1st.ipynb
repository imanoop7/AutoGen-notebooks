{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyautogen\n",
      "  Downloading pyautogen-0.2.35-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting diskcache (from pyautogen)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting docker (from pyautogen)\n",
      "  Using cached docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting flaml (from pyautogen)\n",
      "  Downloading FLAML-2.2.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting numpy<2,>=1.17.0 (from pyautogen)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Collecting openai>=1.3 (from pyautogen)\n",
      "  Downloading openai-1.42.0-py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\anoop maurya\\documents\\github\\autogen-notebooks\\.venv\\lib\\site-packages (from pyautogen) (24.1)\n",
      "Collecting pydantic!=2.6.0,<3,>=1.10 (from pyautogen)\n",
      "  Using cached pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "Collecting python-dotenv (from pyautogen)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting termcolor (from pyautogen)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting tiktoken (from pyautogen)\n",
      "  Using cached tiktoken-0.7.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai>=1.3->pyautogen)\n",
      "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.3->pyautogen)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai>=1.3->pyautogen)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.3->pyautogen)\n",
      "  Downloading jiter-0.5.0-cp312-none-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting sniffio (from openai>=1.3->pyautogen)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai>=1.3->pyautogen)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai>=1.3->pyautogen)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=2.6.0,<3,>=1.10->pyautogen)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic!=2.6.0,<3,>=1.10->pyautogen)\n",
      "  Using cached pydantic_core-2.20.1-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\anoop maurya\\documents\\github\\autogen-notebooks\\.venv\\lib\\site-packages (from docker->pyautogen) (306)\n",
      "Collecting requests>=2.26.0 (from docker->pyautogen)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting urllib3>=1.26.0 (from docker->pyautogen)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken->pyautogen)\n",
      "  Using cached regex-2024.7.24-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai>=1.3->pyautogen)\n",
      "  Downloading idna-3.8-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai>=1.3->pyautogen)\n",
      "  Using cached certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.3->pyautogen)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->pyautogen)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.26.0->docker->pyautogen)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\anoop maurya\\documents\\github\\autogen-notebooks\\.venv\\lib\\site-packages (from tqdm>4->openai>=1.3->pyautogen) (0.4.6)\n",
      "Downloading pyautogen-0.2.35-py3-none-any.whl (318 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Downloading openai-1.42.0-py3-none-any.whl (362 kB)\n",
      "Using cached pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "Using cached pydantic_core-2.20.1-cp312-none-win_amd64.whl (1.9 MB)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Using cached docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Downloading FLAML-2.2.0-py3-none-any.whl (297 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached tiktoken-0.7.0-cp312-cp312-win_amd64.whl (799 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Downloading jiter-0.5.0-cp312-none-win_amd64.whl (189 kB)\n",
      "Using cached regex-2024.7.24-cp312-cp312-win_amd64.whl (269 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Using cached certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "Using cached charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl (100 kB)\n",
      "Downloading idna-3.8-py3-none-any.whl (66 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tqdm, termcolor, sniffio, regex, python-dotenv, numpy, jiter, idna, h11, distro, diskcache, charset-normalizer, certifi, annotated-types, requests, pydantic-core, httpcore, flaml, anyio, tiktoken, pydantic, httpx, docker, openai, pyautogen\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.4.0 certifi-2024.7.4 charset-normalizer-3.3.2 diskcache-5.6.3 distro-1.9.0 docker-7.1.0 flaml-2.2.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 idna-3.8 jiter-0.5.0 numpy-1.26.4 openai-1.42.0 pyautogen-0.2.35 pydantic-2.8.2 pydantic-core-2.20.1 python-dotenv-1.0.1 regex-2024.7.24 requests-2.32.3 sniffio-1.3.1 termcolor-2.4.0 tiktoken-0.7.0 tqdm-4.66.5 typing-extensions-4.12.2 urllib3-2.2.2\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\anoop maurya\\documents\\github\\autogen-notebooks\\.venv\\lib\\site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyautogen\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = [\n",
    "  {\n",
    "    \"model\": \"phi3\",\n",
    "    \"base_url\": \"http://localhost:11434/v1\",\n",
    "    \"api_key\": \"ollama\",\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agents for Pizza and Sushi debate\n",
    "\n",
    "pizza_agent = ConversableAgent(\n",
    "    name=\"pizza_lover\",\n",
    "    system_message=\"You are a person who loves pizza and wants to spread its deliciousness around the world. Speak passionately about the allure of pizza.\",\n",
    "    llm_config={\"config_list\": config_list},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "sushi_agent = ConversableAgent(\n",
    "    name=\"sushi_lover\",\n",
    "    system_message=\"You are a person who loves sushi and wants to spread its deliciousness around the world. Speak passionately about the allure of sushi.\",\n",
    "    llm_config={\"config_list\": config_list},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "judge_agent = ConversableAgent(\n",
    "    name=\"judge_Agent\",\n",
    "    system_message=\"You are acting as the ultimate facilitator. Your job is to guide the debate between the two and declare a winner based on who makes the most convincing argument. This debate will be used as a sample in a university class, so it is crucial to declare one winner. Once a clear conclusion is reached, you must declare 'That's enough!' and announce the winner. The debate cannot end without this phrase, so make sure to include it.\",\n",
    "    llm_config={\"config_list\": config_list},\n",
    "    human_input_mode=\"NEVER\",\n",
    "    is_termination_msg=lambda msg: \"That's enough!\" in msg[\"content\"],\n",
    ")\n",
    "\n",
    "pizza_agent.description = \"The ultimate pizza fan\"\n",
    "sushi_agent.description = \"The ultimate sushi fan\"\n",
    "judge_agent.description = \"The facilitator who decides the debate winner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from autogen import GroupChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_chat = GroupChat(\n",
    "    agents=[pizza_agent, sushi_agent,judge_agent],\n",
    "    messages=[],\n",
    "    send_introductions=True,\n",
    "    speaker_selection_method = \"auto\",\n",
    "    max_round = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import GroupChatManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_chat_manager = GroupChatManager(\n",
    "    groupchat=group_chat,\n",
    "    llm_config={\"config_list\": config_list},\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mjudge_Agent\u001b[0m (to chat_manager):\n",
      "\n",
      "This debate will be used as a sample in a university class. A winner must be decided. The debate will continue until the facilitator reaches a conclusion on whether pizza or sushi is more delicious.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: judge_Agent\n",
      "\u001b[0m\n",
      "\u001b[33mjudge_Agent\u001b[0m (to chat_manager):\n",
      "\n",
      "Welcome to our delicious dessert duel between a seasoned pizza enthusiast, *pizza_lover*, an ardent sushi aficionado, *sushi_lover*, and wise judge, *judge_Agent*. Together, we're going to explore the tastes of Italy and Japan. Remember, you must respect each other's preferences while presenting your arguments as compellingly possible in convincing others that pizza is unmatched or sushi holds the crown for taste supremacy.\n",
      "\n",
      "We will consider several factors such as flavor complexity, cultural influence, ease of preparation and enjoyment, versatility, healthiness, and overall experience. After hearing both perspectives, *judge_Agent* must analyze, evaluate, and declare a winner. This debate has rules - it continues until the facilitator announces that 'It's time to wrap up this delicious debate!'\n",
      "\n",
      "Let's keep the conversation friendly yet passionate. Both sides should present their arguments backed by research or personal experiences when possible.\n",
      "\n",
      "Now, let the heated discussion begin with a strong opening statement from our pizza lover *pizza_lover* and sushi enthusiast *sushi_lover*. After that, we'll dive into rebuttals followed by concluding statements before it's time to declare 'It's enough!'!\n",
      "\n",
      "- - -\n",
      "\n",
      "**[End of Debate Announcement]**\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.client: 08-24 21:07:05] {329} WARNING - Model phi3 is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[32m\n",
      "Next speaker: sushi_lover\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chat_result = judge_agent.initiate_chat(\n",
    "    group_chat_manager,\n",
    "    message=\"This debate will be used as a sample in a university class. A winner must be decided. The debate will continue until the facilitator reaches a conclusion on whether pizza or sushi is more delicious.\",\n",
    "    summary_method=\"reflection_with_llm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
